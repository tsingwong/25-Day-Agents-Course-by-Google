{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Day 10: Big Context â‰  Better Memory\n",
    "\n",
    "**Long-running agent sessions face two enemies: latency and \"lost in the middle\" syndrome. The ADK solves this with Context Caching and Context Compaction.**\n",
    "\n",
    "## è¯¾ç¨‹æ¥æº\n",
    "- [Advent of Agents 2025 - Day 10](https://adventofagents.com/)\n",
    "- [ADK Context Compaction](https://google.github.io/adk-docs/context/compaction/)\n",
    "- [ADK Context Caching](https://google.github.io/adk-docs/context/caching/)\n",
    "\n",
    "## Learning Goals\n",
    "\n",
    "1. ç†è§£ä¸ºä»€ä¹ˆå¤§ä¸Šä¸‹æ–‡ä¸ç­‰äºæ›´å¥½çš„è®°å¿†\n",
    "2. å­¦ä¹  Context Compactionï¼šå¦‚ä½•å‹ç¼©å†å²é˜²æ­¢ context rot\n",
    "3. å­¦ä¹  Context Cachingï¼šå¦‚ä½•ç¼“å­˜é‡æŒ‡ä»¤æå‡æ€§èƒ½\n",
    "4. å®è·µ ADK ä¸­è¿™ä¸¤ç§æŠ€æœ¯çš„å®ç°æ–¹å¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. é—®é¢˜ï¼šBig Context â‰  Better Memory\n",
    "\n",
    "### ä¸¤ä¸ªæ•Œäºº\n",
    "\n",
    "| æ•Œäºº | é—®é¢˜ | åæœ |\n",
    "|------|------|------|\n",
    "| **Latency (å»¶è¿Ÿ)** | ä¸Šä¸‹æ–‡è¶Šé•¿ï¼Œå¤„ç†æ—¶é—´è¶Šä¹… | ç”¨æˆ·ä½“éªŒå·®ï¼Œæˆæœ¬é«˜ |\n",
    "| **Lost in the Middle** | LLM å¯¹é•¿ä¸Šä¸‹æ–‡ä¸­é—´éƒ¨åˆ†å®¹æ˜“é—å¿˜ | å›ç­”ä¸å‡†ç¡®ï¼Œäº§ç”Ÿå¹»è§‰ |\n",
    "\n",
    "### ç ”ç©¶å‘ç°ï¼šLost in the Middle ç°è±¡\n",
    "\n",
    "```\n",
    "Retrieval Accuracy by Position in Context:\n",
    "\n",
    "100% â”¤â–ˆâ–ˆ                              â–ˆâ–ˆ\n",
    " 90% â”¤â–ˆâ–ˆ                              â–ˆâ–ˆ\n",
    " 80% â”¤â–ˆâ–ˆ                              â–ˆâ–ˆ\n",
    " 70% â”¤â–ˆâ–ˆ â–ˆ                          â–ˆ â–ˆâ–ˆ\n",
    " 60% â”¤â–ˆâ–ˆ â–ˆâ–ˆ                        â–ˆâ–ˆ â–ˆâ–ˆ\n",
    " 50% â”¤â–ˆâ–ˆ â–ˆâ–ˆ â–ˆ                    â–ˆ â–ˆâ–ˆ â–ˆâ–ˆ\n",
    " 40% â”¤â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆ              â–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ\n",
    " 30% â”¤â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆ        â–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ\n",
    " 20% â”¤â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆ  â–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ\n",
    " 10% â”¤â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ â–ˆâ–ˆ\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "      Start                         End\n",
    "           Position in Context\n",
    "```\n",
    "\n",
    "> LLM å¯¹å¼€å¤´å’Œç»“å°¾çš„ä¿¡æ¯è®°å¿†æœ€å¥½ï¼Œä¸­é—´éƒ¨åˆ†å®¹æ˜“è¢«\"é—å¿˜\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. ADK çš„è§£å†³æ–¹æ¡ˆ\n",
    "\n",
    "ADK æä¾›ä¸¤ä¸ªäº’è¡¥çš„æŠ€æœ¯ï¼š\n",
    "\n",
    "| æŠ€æœ¯ | ç›®çš„ | è§£å†³çš„é—®é¢˜ |\n",
    "|------|------|------------|\n",
    "| **Context Compaction** | å‹ç¼©/æ‘˜è¦å†å²å¯¹è¯ | Lost in the Middle, å»¶è¿Ÿ |\n",
    "| **Context Caching** | ç¼“å­˜ç¨³å®šçš„æŒ‡ä»¤å‰ç¼€ | æˆæœ¬, å»¶è¿Ÿ |\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    WORKING CONTEXT                          â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\n",
    "â”‚  â”‚ Stable Prefix   â”‚ Variable Suffix                  â”‚    â”‚\n",
    "â”‚  â”‚ (Context Cache) â”‚ (Context Compaction)             â”‚    â”‚\n",
    "â”‚  â”‚                 â”‚                                  â”‚    â”‚\n",
    "â”‚  â”‚ â€¢ System prompt â”‚ â€¢ Compacted history summary      â”‚    â”‚\n",
    "â”‚  â”‚ â€¢ Agent identityâ”‚ â€¢ Recent conversation turns      â”‚    â”‚\n",
    "â”‚  â”‚ â€¢ Tools/Schema  â”‚ â€¢ Current user message           â”‚    â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®‰è£…ä¾èµ–\n",
    "# !uv pip install google-adk google-genai python-dotenv\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from shared import get_api_key\n",
    "\n",
    "# åŠ è½½ API key\n",
    "GOOGLE_API_KEY = get_api_key()\n",
    "print(\"âœ… API Key loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Context Compactionï¼ˆä¸Šä¸‹æ–‡å‹ç¼©ï¼‰\n",
    "\n",
    "### ä»€ä¹ˆæ˜¯ Context Compactionï¼Ÿ\n",
    "\n",
    "å½“å¯¹è¯å†å²å˜å¾—å¤ªé•¿æ—¶ï¼Œè‡ªåŠ¨å°†æ—§çš„å¯¹è¯å‹ç¼©æˆæ‘˜è¦ï¼Œä¿æŒä¸Šä¸‹æ–‡åœ¨å¯æ§èŒƒå›´å†…ã€‚\n",
    "\n",
    "### ä¸ºä»€ä¹ˆéœ€è¦ï¼Ÿ\n",
    "\n",
    "```python\n",
    "# âŒ Without Compaction: Context grows unboundedly\n",
    "Turn 1:   [System] + [User1] + [Agent1]                    = 1000 tokens\n",
    "Turn 10:  [System] + [User1..10] + [Agent1..10]            = 10000 tokens\n",
    "Turn 100: [System] + [User1..100] + [Agent1..100]          = 100000 tokens ğŸ’¥\n",
    "\n",
    "# âœ… With Compaction: Context stays bounded\n",
    "Turn 1:   [System] + [User1] + [Agent1]                    = 1000 tokens\n",
    "Turn 10:  [System] + [Summary1-5] + [User6..10] + [Agent6..10] = 3000 tokens\n",
    "Turn 100: [System] + [Summary1-95] + [User96..100] + [Agent96..100] = 3000 tokens âœ…\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Compaction å·¥ä½œåŸç†\n",
    "\n",
    "```\n",
    "Before Compaction:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ [System] [T1] [T2] [T3] [T4] [T5] [T6] [T7] [T8] [T9] [T10] â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                           â†“\n",
    "                     LLM Summarization\n",
    "                           â†“\n",
    "After Compaction:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ [System] [Summary T1-T7] [T8] [T9] [T10]       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### å…³é”®æ¦‚å¿µ\n",
    "\n",
    "| æ¦‚å¿µ | è¯´æ˜ |\n",
    "|------|------|\n",
    "| **Compaction Threshold** | è§¦å‘å‹ç¼©çš„ token é˜ˆå€¼ |\n",
    "| **Recent Turns to Keep** | ä¿ç•™æœ€è¿‘ N è½®ä¸å‹ç¼© |\n",
    "| **Summary Model** | ç”¨äºç”Ÿæˆæ‘˜è¦çš„æ¨¡å‹ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ‹Ÿ Context Compaction çš„æ¦‚å¿µ\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "class ConversationTurn:\n",
    "    \"\"\"å•è½®å¯¹è¯\"\"\"\n",
    "    def __init__(self, user: str, agent: str, tokens: int = 100):\n",
    "        self.user = user\n",
    "        self.agent = agent\n",
    "        self.tokens = tokens\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"Turn(U: {self.user[:20]}... A: {self.agent[:20]}...)\"\n",
    "\n",
    "\n",
    "class MockCompactor:\n",
    "    \"\"\"æ¨¡æ‹Ÿ ADK çš„ Context Compaction é€»è¾‘\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        max_tokens: int = 4000,\n",
    "        recent_turns_to_keep: int = 3,\n",
    "        system_tokens: int = 500\n",
    "    ):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.recent_turns_to_keep = recent_turns_to_keep\n",
    "        self.system_tokens = system_tokens\n",
    "        self.summary = None\n",
    "        self.summary_tokens = 0\n",
    "    \n",
    "    def calculate_tokens(self, turns: List[ConversationTurn]) -> int:\n",
    "        \"\"\"è®¡ç®—å½“å‰ä¸Šä¸‹æ–‡å¤§å°\"\"\"\n",
    "        turn_tokens = sum(t.tokens for t in turns)\n",
    "        return self.system_tokens + self.summary_tokens + turn_tokens\n",
    "    \n",
    "    def needs_compaction(self, turns: List[ConversationTurn]) -> bool:\n",
    "        \"\"\"æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©\"\"\"\n",
    "        return self.calculate_tokens(turns) > self.max_tokens\n",
    "    \n",
    "    def compact(self, turns: List[ConversationTurn]) -> tuple:\n",
    "        \"\"\"æ‰§è¡Œå‹ç¼©ï¼Œè¿”å› (summary, remaining_turns)\"\"\"\n",
    "        if not self.needs_compaction(turns):\n",
    "            return self.summary, turns\n",
    "        \n",
    "        # ä¿ç•™æœ€è¿‘ N è½®\n",
    "        keep_turns = turns[-self.recent_turns_to_keep:]\n",
    "        compact_turns = turns[:-self.recent_turns_to_keep]\n",
    "        \n",
    "        # ç”Ÿæˆæ‘˜è¦ï¼ˆæ¨¡æ‹Ÿ LLM è°ƒç”¨ï¼‰\n",
    "        old_summary = self.summary or \"\"\n",
    "        new_content = \"\\n\".join(\n",
    "            f\"- User asked about {t.user[:30]}, Agent responded about {t.agent[:30]}\"\n",
    "            for t in compact_turns\n",
    "        )\n",
    "        \n",
    "        self.summary = f\"\"\"Previous conversation summary:\n",
    "{old_summary}\n",
    "{new_content}\"\"\".strip()\n",
    "        \n",
    "        # æ‘˜è¦é€šå¸¸æ¯”åŸå†…å®¹å°å¾ˆå¤š\n",
    "        compacted_tokens = sum(t.tokens for t in compact_turns)\n",
    "        self.summary_tokens = int(compacted_tokens * 0.2)  # ~20% of original\n",
    "        \n",
    "        print(f\"ğŸ“¦ Compacted {len(compact_turns)} turns ({compacted_tokens} tokens) -> Summary ({self.summary_tokens} tokens)\")\n",
    "        \n",
    "        return self.summary, keep_turns\n",
    "\n",
    "print(\"âœ… MockCompactor class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¼”ç¤º Compaction è¿‡ç¨‹\n",
    "\n",
    "compactor = MockCompactor(\n",
    "    max_tokens=2000,      # ä¸Šä¸‹æ–‡é™åˆ¶ 2000 tokens\n",
    "    recent_turns_to_keep=3,  # ä¿ç•™æœ€è¿‘ 3 è½®\n",
    "    system_tokens=500     # System prompt 500 tokens\n",
    ")\n",
    "\n",
    "# æ¨¡æ‹Ÿå¯¹è¯ç§¯ç´¯\n",
    "conversation: List[ConversationTurn] = []\n",
    "\n",
    "sample_turns = [\n",
    "    (\"What products do you have?\", \"We have electronics, clothing, and home goods.\"),\n",
    "    (\"Show me laptops under $1000\", \"Here are 5 laptops: MacBook Air, Dell XPS...\"),\n",
    "    (\"Tell me more about the MacBook\", \"The MacBook Air M3 has 8GB RAM, 256GB SSD...\"),\n",
    "    (\"What's the warranty?\", \"Apple offers 1-year limited warranty...\"),\n",
    "    (\"Can I get extended warranty?\", \"Yes, AppleCare+ is available for $149...\"),\n",
    "    (\"Add MacBook to cart\", \"Added MacBook Air M3 to your cart. Total: $999\"),\n",
    "    (\"What payment methods?\", \"We accept credit cards, PayPal, Apple Pay...\"),\n",
    "    (\"Do you have student discounts?\", \"Yes! Students get 10% off with valid ID.\"),\n",
    "    (\"Apply student discount\", \"Student discount applied! New total: $899.10\"),\n",
    "    (\"Proceed to checkout\", \"Redirecting to secure checkout...\"),\n",
    "]\n",
    "\n",
    "print(\"Simulating 10-turn conversation...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (user_msg, agent_msg) in enumerate(sample_turns, 1):\n",
    "    turn = ConversationTurn(user_msg, agent_msg, tokens=150)\n",
    "    conversation.append(turn)\n",
    "    \n",
    "    current_tokens = compactor.calculate_tokens(conversation)\n",
    "    print(f\"\\nTurn {i}: {current_tokens} tokens\")\n",
    "    \n",
    "    if compactor.needs_compaction(conversation):\n",
    "        print(f\"âš ï¸  Exceeded {compactor.max_tokens} tokens, triggering compaction...\")\n",
    "        summary, conversation = compactor.compact(conversation)\n",
    "        print(f\"âœ… After compaction: {compactor.calculate_tokens(conversation)} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹æœ€ç»ˆçŠ¶æ€\n",
    "print(\"Final Context Structure:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\n[System Prompt]: {compactor.system_tokens} tokens\")\n",
    "print(f\"\\n[Summary]: {compactor.summary_tokens} tokens\")\n",
    "if compactor.summary:\n",
    "    print(compactor.summary)\n",
    "print(f\"\\n[Recent Turns]: {len(conversation)} turns\")\n",
    "for i, turn in enumerate(conversation):\n",
    "    print(f\"  {i+1}. U: {turn.user[:40]}...\")\n",
    "    print(f\"     A: {turn.agent[:40]}...\")\n",
    "print(f\"\\nğŸ“Š Total: {compactor.calculate_tokens(conversation)} tokens (limit: {compactor.max_tokens})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. ADK ä¸­çš„ Context Compaction\n",
    "\n",
    "ADK æä¾›å†…ç½®çš„ Compaction æ”¯æŒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.agents import Agent\n",
    "from google.adk.agents.run_config import RunConfig\n",
    "\n",
    "# åˆ›å»ºå¸¦æœ‰ Compaction é…ç½®çš„ Agent\n",
    "agent_with_compaction = Agent(\n",
    "    name=\"support_agent\",\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    instruction=\"You are a helpful customer support agent.\",\n",
    ")\n",
    "\n",
    "# RunConfig ä¸­é…ç½® Compaction\n",
    "run_config = RunConfig(\n",
    "    # å½“ session äº‹ä»¶è¾¾åˆ°æ­¤æ•°é‡æ—¶è§¦å‘å‹ç¼©\n",
    "    max_llm_calls=50,  # æœ€å¤§ LLM è°ƒç”¨æ¬¡æ•°\n",
    ")\n",
    "\n",
    "print(\"âœ… Agent with compaction config created\")\n",
    "print(f\"   Max LLM calls: {run_config.max_llm_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "### Compaction é…ç½®é€‰é¡¹\n",
    "\n",
    "```python\n",
    "# ADK çš„å‹ç¼©é…ç½®ï¼ˆæ¦‚å¿µç¤ºä¾‹ï¼‰\n",
    "compaction_config = {\n",
    "    \"enabled\": True,\n",
    "    \"threshold_tokens\": 8000,      # è§¦å‘å‹ç¼©çš„ token é˜ˆå€¼\n",
    "    \"target_tokens\": 4000,         # å‹ç¼©åçš„ç›®æ ‡å¤§å°\n",
    "    \"recent_turns_to_keep\": 5,     # ä¿ç•™æœ€è¿‘ N è½®ä¸å‹ç¼©\n",
    "    \"summarization_model\": \"gemini-2.0-flash\",  # ç”¨äºç”Ÿæˆæ‘˜è¦çš„æ¨¡å‹\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Context Cachingï¼ˆä¸Šä¸‹æ–‡ç¼“å­˜ï¼‰\n",
    "\n",
    "### ä»€ä¹ˆæ˜¯ Context Cachingï¼Ÿ\n",
    "\n",
    "å°†ç¨³å®šä¸å˜çš„ä¸Šä¸‹æ–‡å‰ç¼€ï¼ˆå¦‚ System Promptã€å·¥å…·å®šä¹‰ï¼‰ç¼“å­˜åœ¨ API ç«¯ï¼Œé¿å…æ¯æ¬¡è¯·æ±‚é‡å¤å¤„ç†ã€‚\n",
    "\n",
    "### Static vs Turn Instructions å›é¡¾ï¼ˆDay 8ï¼‰\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  STABLE PREFIX (å¯ç¼“å­˜)                  â”‚\n",
    "â”‚  â”œâ”€ System Prompt                        â”‚\n",
    "â”‚  â”œâ”€ Agent Identity                       â”‚\n",
    "â”‚  â”œâ”€ Tool Definitions                     â”‚\n",
    "â”‚  â””â”€ Few-shot Examples                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  VARIABLE SUFFIX (æ¯æ¬¡è¯·æ±‚åŠ¨æ€)          â”‚\n",
    "â”‚  â”œâ”€ Turn Instructions                    â”‚\n",
    "â”‚  â”œâ”€ Retrieved Memory                     â”‚\n",
    "â”‚  â”œâ”€ Compacted History                    â”‚\n",
    "â”‚  â””â”€ Current User Message                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### Caching çš„æ”¶ç›Š\n",
    "\n",
    "```\n",
    "Without Caching:\n",
    "Request 1: Process [System: 2000 tokens] + [User: 100 tokens] = 2100 tokens processed\n",
    "Request 2: Process [System: 2000 tokens] + [User: 150 tokens] = 2150 tokens processed\n",
    "Request 3: Process [System: 2000 tokens] + [User: 120 tokens] = 2120 tokens processed\n",
    "Total: 6370 tokens processed\n",
    "\n",
    "With Caching:\n",
    "Request 1: Process [System: 2000 tokens] + [User: 100 tokens] = 2100 tokens (cache miss)\n",
    "Request 2: Cache hit [System] + Process [User: 150 tokens]    = 150 tokens processed\n",
    "Request 3: Cache hit [System] + Process [User: 120 tokens]    = 120 tokens processed\n",
    "Total: 2370 tokens processed (~63% reduction!)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¨¡æ‹Ÿ Context Caching çš„æˆæœ¬è®¡ç®—\n",
    "\n",
    "class CachingCalculator:\n",
    "    \"\"\"è®¡ç®— Context Caching çš„æˆæœ¬æ”¶ç›Š\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        static_tokens: int,\n",
    "        input_price_per_1k: float = 0.075,  # Gemini 2.0 Flash ä»·æ ¼\n",
    "        cached_price_per_1k: float = 0.01875,  # ç¼“å­˜å‘½ä¸­ä»·æ ¼ (é€šå¸¸æ˜¯ 1/4)\n",
    "    ):\n",
    "        self.static_tokens = static_tokens\n",
    "        self.input_price = input_price_per_1k / 1000\n",
    "        self.cached_price = cached_price_per_1k / 1000\n",
    "    \n",
    "    def calculate_savings(self, num_requests: int, avg_dynamic_tokens: int) -> dict:\n",
    "        \"\"\"è®¡ç®— N æ¬¡è¯·æ±‚çš„æˆæœ¬å¯¹æ¯”\"\"\"\n",
    "        # æ— ç¼“å­˜\n",
    "        no_cache_tokens = num_requests * (self.static_tokens + avg_dynamic_tokens)\n",
    "        no_cache_cost = no_cache_tokens * self.input_price\n",
    "        \n",
    "        # æœ‰ç¼“å­˜ï¼ˆç¬¬ä¸€æ¬¡ missï¼Œåç»­ hitï¼‰\n",
    "        first_request = (self.static_tokens + avg_dynamic_tokens) * self.input_price\n",
    "        cached_requests = (num_requests - 1) * (\n",
    "            self.static_tokens * self.cached_price +  # ç¼“å­˜å‘½ä¸­éƒ¨åˆ†\n",
    "            avg_dynamic_tokens * self.input_price      # åŠ¨æ€éƒ¨åˆ†\n",
    "        )\n",
    "        with_cache_cost = first_request + cached_requests\n",
    "        \n",
    "        savings = no_cache_cost - with_cache_cost\n",
    "        savings_pct = (savings / no_cache_cost) * 100 if no_cache_cost > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"requests\": num_requests,\n",
    "            \"no_cache_tokens\": no_cache_tokens,\n",
    "            \"no_cache_cost\": no_cache_cost,\n",
    "            \"with_cache_cost\": with_cache_cost,\n",
    "            \"savings\": savings,\n",
    "            \"savings_pct\": savings_pct\n",
    "        }\n",
    "\n",
    "# ç¤ºä¾‹ï¼š2000 token çš„ system prompt\n",
    "calc = CachingCalculator(static_tokens=2000)\n",
    "\n",
    "print(\"Context Caching Cost Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Static Prefix: 2000 tokens\")\n",
    "print(f\"Avg Dynamic: 500 tokens/request\")\n",
    "print()\n",
    "\n",
    "for num_requests in [10, 100, 1000]:\n",
    "    result = calc.calculate_savings(num_requests, avg_dynamic_tokens=500)\n",
    "    print(f\"\\n{num_requests} requests:\")\n",
    "    print(f\"  Without cache: ${result['no_cache_cost']:.4f}\")\n",
    "    print(f\"  With cache:    ${result['with_cache_cost']:.4f}\")\n",
    "    print(f\"  Savings:       ${result['savings']:.4f} ({result['savings_pct']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. ADK ä¸­å¯ç”¨ Context Caching\n",
    "\n",
    "ADK æ”¯æŒé€šè¿‡ Gemini API çš„ Context Caching åŠŸèƒ½ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADK ä¸­ä½¿ç”¨ Context Caching çš„æ¦‚å¿µç¤ºä¾‹\n",
    "\n",
    "# å¤§å‹ System Promptï¼ˆé€‚åˆç¼“å­˜ï¼‰\n",
    "LARGE_SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert financial advisor AI assistant.\n",
    "\n",
    "## Your Expertise\n",
    "- Investment portfolio management\n",
    "- Retirement planning\n",
    "- Tax optimization strategies\n",
    "- Risk assessment and management\n",
    "- Market analysis and trends\n",
    "\n",
    "## Communication Guidelines\n",
    "1. Always provide balanced perspectives\n",
    "2. Clearly state assumptions and limitations\n",
    "3. Use data and evidence to support recommendations\n",
    "4. Explain complex concepts in accessible terms\n",
    "5. Never provide specific investment advice without disclaimers\n",
    "\n",
    "## Response Format\n",
    "Structure your responses as follows:\n",
    "- **Summary**: Brief overview (1-2 sentences)\n",
    "- **Analysis**: Detailed breakdown\n",
    "- **Recommendations**: Actionable next steps\n",
    "- **Risks**: Potential downsides to consider\n",
    "- **Disclaimer**: Appropriate legal/financial disclaimers\n",
    "\n",
    "## Regulatory Compliance\n",
    "- Always remind users to consult licensed professionals\n",
    "- Never guarantee specific returns\n",
    "- Disclose that AI advice is educational, not professional\n",
    "- Follow SEC and FINRA guidelines for financial communication\n",
    "\n",
    "## Tool Usage\n",
    "You have access to the following tools:\n",
    "- calculate_compound_interest(principal, rate, years)\n",
    "- get_stock_price(ticker)\n",
    "- analyze_portfolio(holdings)\n",
    "- calculate_tax_impact(income, deductions)\n",
    "\n",
    "[... æ›´å¤šè¯¦ç»†è¯´æ˜ ...]\n",
    "\"\"\" * 3  # æ¨¡æ‹Ÿæ›´é•¿çš„ prompt\n",
    "\n",
    "print(f\"System Prompt size: ~{len(LARGE_SYSTEM_PROMPT.split())} words\")\n",
    "print(f\"Estimated tokens: ~{len(LARGE_SYSTEM_PROMPT) // 4} tokens\")\n",
    "print(\"\\nâœ… This large prompt is ideal for Context Caching!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.adk.agents import Agent\n",
    "\n",
    "# å·¥å…·å®šä¹‰\n",
    "def calculate_compound_interest(principal: float, rate: float, years: int) -> dict:\n",
    "    \"\"\"Calculate compound interest.\n",
    "    \n",
    "    Args:\n",
    "        principal: Initial investment amount\n",
    "        rate: Annual interest rate (as decimal, e.g., 0.07 for 7%)\n",
    "        years: Number of years\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with final amount and total interest\n",
    "    \"\"\"\n",
    "    final_amount = principal * (1 + rate) ** years\n",
    "    total_interest = final_amount - principal\n",
    "    return {\n",
    "        \"principal\": principal,\n",
    "        \"final_amount\": round(final_amount, 2),\n",
    "        \"total_interest\": round(total_interest, 2),\n",
    "        \"years\": years,\n",
    "        \"rate\": f\"{rate * 100}%\"\n",
    "    }\n",
    "\n",
    "def get_stock_price(ticker: str) -> dict:\n",
    "    \"\"\"Get current stock price (mock).\"\"\"\n",
    "    # æ¨¡æ‹Ÿæ•°æ®\n",
    "    prices = {\n",
    "        \"AAPL\": 195.50,\n",
    "        \"GOOGL\": 175.25,\n",
    "        \"MSFT\": 425.80,\n",
    "        \"AMZN\": 185.30\n",
    "    }\n",
    "    price = prices.get(ticker.upper(), 100.00)\n",
    "    return {\"ticker\": ticker.upper(), \"price\": price, \"currency\": \"USD\"}\n",
    "\n",
    "\n",
    "# åˆ›å»º Agentï¼ˆStatic Instructions ä¼šè¢«ç¼“å­˜ï¼‰\n",
    "financial_advisor = Agent(\n",
    "    name=\"financial_advisor\",\n",
    "    model=\"gemini-2.0-flash\",\n",
    "    instruction=LARGE_SYSTEM_PROMPT,  # è¿™ä¸ªå¤§ prompt é€‚åˆç¼“å­˜\n",
    "    tools=[calculate_compound_interest, get_stock_price],\n",
    ")\n",
    "\n",
    "print(\"âœ… Financial Advisor Agent created\")\n",
    "print(\"   The large system prompt will benefit from API-level caching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "### Caching æœ€ä½³å®è·µ\n",
    "\n",
    "| âœ… Do | âŒ Don't |\n",
    "|-------|----------|\n",
    "| ç¼“å­˜å¤§å‹ã€ç¨³å®šçš„ system prompts | ç¼“å­˜ç»å¸¸å˜åŒ–çš„å†…å®¹ |\n",
    "| ç¼“å­˜å·¥å…·å®šä¹‰å’Œ schema | ç¼“å­˜ç”¨æˆ·ç‰¹å®šçš„æ•°æ® |\n",
    "| ç¼“å­˜ few-shot ç¤ºä¾‹ | ç¼“å­˜æ—¶é—´æ•æ„Ÿçš„ä¿¡æ¯ |\n",
    "| è®¾ç½®åˆç†çš„ TTL (30-90åˆ†é’Ÿ) | ç¼“å­˜å¤ªå°çš„å†…å®¹ (<1000 tokens) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. ç»“åˆä½¿ç”¨ï¼šCaching + Compaction\n",
    "\n",
    "ä¸¤ç§æŠ€æœ¯å¯ä»¥å®Œç¾é…åˆä½¿ç”¨ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®Œæ•´çš„ä¸Šä¸‹æ–‡ç®¡ç†ç­–ç•¥\n",
    "\n",
    "class ContextManager:\n",
    "    \"\"\"ç»“åˆ Caching å’Œ Compaction çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        system_prompt: str,\n",
    "        max_context_tokens: int = 8000,\n",
    "        compaction_threshold: int = 6000,\n",
    "        recent_turns_to_keep: int = 5\n",
    "    ):\n",
    "        self.system_prompt = system_prompt\n",
    "        self.system_tokens = len(system_prompt) // 4  # ä¼°ç®—\n",
    "        self.max_context = max_context_tokens\n",
    "        self.compaction_threshold = compaction_threshold\n",
    "        self.recent_turns = recent_turns_to_keep\n",
    "        \n",
    "        self.history_summary = None\n",
    "        self.summary_tokens = 0\n",
    "        self.conversation = []\n",
    "        self.cache_hits = 0\n",
    "        self.total_requests = 0\n",
    "    \n",
    "    def build_context(self, user_message: str, turn_instructions: str = \"\") -> dict:\n",
    "        \"\"\"æ„å»ºæœ€ç»ˆå‘é€ç»™ LLM çš„ä¸Šä¸‹æ–‡\"\"\"\n",
    "        self.total_requests += 1\n",
    "        \n",
    "        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‹ç¼©\n",
    "        current_tokens = self._estimate_tokens()\n",
    "        if current_tokens > self.compaction_threshold:\n",
    "            self._compact()\n",
    "        \n",
    "        # æ„å»ºä¸Šä¸‹æ–‡\n",
    "        context = {\n",
    "            \"system\": self.system_prompt,  # <- ä¼šè¢« API ç¼“å­˜\n",
    "            \"messages\": []\n",
    "        }\n",
    "        \n",
    "        # æ·»åŠ å†å²æ‘˜è¦ï¼ˆå¦‚æœæœ‰ï¼‰\n",
    "        if self.history_summary:\n",
    "            context[\"messages\"].append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"[Previous conversation summary]\\n{self.history_summary}\"\n",
    "            })\n",
    "        \n",
    "        # æ·»åŠ æœ€è¿‘å¯¹è¯\n",
    "        for turn in self.conversation:\n",
    "            context[\"messages\"].append({\"role\": \"user\", \"content\": turn[\"user\"]})\n",
    "            if \"agent\" in turn:\n",
    "                context[\"messages\"].append({\"role\": \"assistant\", \"content\": turn[\"agent\"]})\n",
    "        \n",
    "        # æ·»åŠ  turn instructions + æ–°æ¶ˆæ¯\n",
    "        if turn_instructions:\n",
    "            context[\"messages\"].append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"[Turn Instructions]\\n{turn_instructions}\"\n",
    "            })\n",
    "        \n",
    "        context[\"messages\"].append({\"role\": \"user\", \"content\": user_message})\n",
    "        \n",
    "        # æ¨¡æ‹Ÿç¼“å­˜å‘½ä¸­ï¼ˆé™¤äº†ç¬¬ä¸€æ¬¡ï¼‰\n",
    "        if self.total_requests > 1:\n",
    "            self.cache_hits += 1\n",
    "        \n",
    "        return context\n",
    "    \n",
    "    def add_turn(self, user: str, agent: str):\n",
    "        \"\"\"æ·»åŠ ä¸€è½®å¯¹è¯\"\"\"\n",
    "        self.conversation.append({\"user\": user, \"agent\": agent})\n",
    "    \n",
    "    def _estimate_tokens(self) -> int:\n",
    "        \"\"\"ä¼°ç®—å½“å‰ä¸Šä¸‹æ–‡å¤§å°\"\"\"\n",
    "        conv_tokens = sum(len(str(t)) // 4 for t in self.conversation)\n",
    "        return self.system_tokens + self.summary_tokens + conv_tokens\n",
    "    \n",
    "    def _compact(self):\n",
    "        \"\"\"æ‰§è¡Œå‹ç¼©\"\"\"\n",
    "        if len(self.conversation) <= self.recent_turns:\n",
    "            return\n",
    "        \n",
    "        to_compact = self.conversation[:-self.recent_turns]\n",
    "        self.conversation = self.conversation[-self.recent_turns:]\n",
    "        \n",
    "        # ç”Ÿæˆæ‘˜è¦\n",
    "        summary_parts = []\n",
    "        if self.history_summary:\n",
    "            summary_parts.append(self.history_summary)\n",
    "        \n",
    "        summary_parts.append(\"Recent topics discussed:\")\n",
    "        for turn in to_compact:\n",
    "            summary_parts.append(f\"- {turn['user'][:50]}...\")\n",
    "        \n",
    "        self.history_summary = \"\\n\".join(summary_parts)\n",
    "        self.summary_tokens = len(self.history_summary) // 4\n",
    "        \n",
    "        print(f\"ğŸ“¦ Compacted {len(to_compact)} turns into summary\")\n",
    "    \n",
    "    def get_stats(self) -> dict:\n",
    "        \"\"\"è·å–ç»Ÿè®¡ä¿¡æ¯\"\"\"\n",
    "        return {\n",
    "            \"total_requests\": self.total_requests,\n",
    "            \"cache_hits\": self.cache_hits,\n",
    "            \"cache_hit_rate\": f\"{(self.cache_hits / max(1, self.total_requests)) * 100:.1f}%\",\n",
    "            \"current_context_tokens\": self._estimate_tokens(),\n",
    "            \"active_turns\": len(self.conversation),\n",
    "            \"has_summary\": self.history_summary is not None\n",
    "        }\n",
    "\n",
    "print(\"âœ… ContextManager class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ¼”ç¤ºå®Œæ•´æµç¨‹\n",
    "\n",
    "# ä½¿ç”¨è¾ƒå°çš„é˜ˆå€¼ä¾¿äºæ¼”ç¤º\n",
    "ctx_manager = ContextManager(\n",
    "    system_prompt=LARGE_SYSTEM_PROMPT,\n",
    "    max_context_tokens=4000,\n",
    "    compaction_threshold=2000,\n",
    "    recent_turns_to_keep=3\n",
    ")\n",
    "\n",
    "# æ¨¡æ‹Ÿå¤šè½®å¯¹è¯\n",
    "conversations = [\n",
    "    (\"What's a good retirement savings strategy?\", \"For retirement, consider a diversified approach...\"),\n",
    "    (\"How much should I save monthly?\", \"A common rule is 15% of your income...\"),\n",
    "    (\"What about 401k vs IRA?\", \"Both are excellent options. 401k offers higher limits...\"),\n",
    "    (\"Calculate compound interest for $10000 at 7% for 30 years\", \"Using the calculator: $76,122.55\"),\n",
    "    (\"What's AAPL stock price?\", \"Apple (AAPL) is currently at $195.50\"),\n",
    "    (\"Should I invest in index funds?\", \"Index funds are great for passive investors...\"),\n",
    "    (\"What are the risks?\", \"Main risks include market volatility and...\"),\n",
    "    (\"Summarize our discussion\", \"We covered retirement strategies, savings rates...\"),\n",
    "]\n",
    "\n",
    "print(\"Simulating conversation with Caching + Compaction...\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, (user_msg, agent_response) in enumerate(conversations, 1):\n",
    "    print(f\"\\n--- Turn {i} ---\")\n",
    "    \n",
    "    # æ„å»ºä¸Šä¸‹æ–‡ï¼ˆä¼šè§¦å‘ç¼“å­˜å’Œå‹ç¼©ï¼‰\n",
    "    context = ctx_manager.build_context(\n",
    "        user_message=user_msg,\n",
    "        turn_instructions=f\"Current turn: {i}\"\n",
    "    )\n",
    "    \n",
    "    # è®°å½•å¯¹è¯\n",
    "    ctx_manager.add_turn(user_msg, agent_response)\n",
    "    \n",
    "    # æ˜¾ç¤ºçŠ¶æ€\n",
    "    stats = ctx_manager.get_stats()\n",
    "    print(f\"User: {user_msg[:50]}...\")\n",
    "    print(f\"Context: {stats['current_context_tokens']} tokens, Cache hits: {stats['cache_hits']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ€ç»ˆç»Ÿè®¡\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FINAL STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "stats = ctx_manager.get_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\nğŸ“Š Benefits achieved:\")\n",
    "print(f\"  âœ… Cache hit rate: {stats['cache_hit_rate']}\")\n",
    "print(f\"  âœ… Context bounded at ~{stats['current_context_tokens']} tokens\")\n",
    "print(f\"  âœ… Compaction: {stats['has_summary']}\")\n",
    "print(f\"  âœ… Active turns kept: {stats['active_turns']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Key Takeaways\n",
    "\n",
    "### Big Context â‰  Better Memory\n",
    "\n",
    "| é—®é¢˜ | è§£å†³æ–¹æ¡ˆ |\n",
    "|------|----------|\n",
    "| Lost in the Middle | Context Compaction ä¿æŒä¸Šä¸‹æ–‡ç²¾ç®€ |\n",
    "| å»¶è¿Ÿå¢åŠ  | ä¸¤è€…ç»“åˆå‡å°‘å¤„ç†é‡ |\n",
    "| æˆæœ¬å¢é•¿ | Context Caching å¤ç”¨è®¡ç®— |\n",
    "\n",
    "### Context Compaction\n",
    "\n",
    "- **ä½•æ—¶ä½¿ç”¨**: é•¿å¯¹è¯ã€å¤šè½®äº¤äº’\n",
    "- **å·¥ä½œæ–¹å¼**: è‡ªåŠ¨æ‘˜è¦æ—§å¯¹è¯ï¼Œä¿ç•™æœ€è¿‘ N è½®\n",
    "- **æ”¶ç›Š**: é˜²æ­¢ä¸Šä¸‹æ–‡æ— é™å¢é•¿ï¼Œä¿æŒå“åº”è´¨é‡\n",
    "\n",
    "### Context Caching  \n",
    "\n",
    "- **ä½•æ—¶ä½¿ç”¨**: å¤§å‹ã€ç¨³å®šçš„ system prompts\n",
    "- **å·¥ä½œæ–¹å¼**: API ç¼“å­˜å‰ç¼€ï¼Œåç»­è¯·æ±‚å¤ç”¨\n",
    "- **æ”¶ç›Š**: é™ä½æˆæœ¬ 60%+ï¼Œå‡å°‘å»¶è¿Ÿ\n",
    "\n",
    "### æœ€ä½³å®è·µ\n",
    "\n",
    "```python\n",
    "# 1. åˆ†ç¦» Static å’Œ Dynamic å†…å®¹\n",
    "static_prompt = \"...large, stable content...\"  # å¯ç¼“å­˜\n",
    "turn_instructions = f\"...dynamic, per-request...\"  # åŠ¨æ€ç”Ÿæˆ\n",
    "\n",
    "# 2. è®¾ç½®åˆç†çš„ Compaction é˜ˆå€¼\n",
    "compaction_config = {\n",
    "    \"threshold\": 8000,  # è§¦å‘ç‚¹\n",
    "    \"target\": 4000,     # ç›®æ ‡å¤§å°\n",
    "    \"keep_recent\": 5    # ä¿ç•™æœ€è¿‘ N è½®\n",
    "}\n",
    "\n",
    "# 3. ç›‘æ§æŒ‡æ ‡\n",
    "- Cache hit rate\n",
    "- Average context size\n",
    "- Compaction frequency\n",
    "- Response quality over time\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Resources\n",
    "\n",
    "### å®˜æ–¹æ–‡æ¡£\n",
    "- [ADK Context Compaction](https://google.github.io/adk-docs/context/compaction/) - å‹ç¼©å†å²é˜²æ­¢ context rot\n",
    "- [ADK Context Caching](https://google.github.io/adk-docs/context/caching/) - ç¼“å­˜é‡æŒ‡ä»¤æå‡æ€§èƒ½\n",
    "- [ADK Context Documentation](https://google.github.io/adk-docs/context/) - ä¸Šä¸‹æ–‡ç®¡ç†æ€»è§ˆ\n",
    "\n",
    "### ç›¸å…³é˜…è¯»\n",
    "- [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172) - ç ”ç©¶è®ºæ–‡\n",
    "- [Context Compaction Explained (Visual)](https://x.com/Saboo_Shubham_/status/1978286461607911617) - å¯è§†åŒ–è§£é‡Š\n",
    "- [ADK Context Caching & Compaction Infographic](https://adventofagents.com/aoa-day10-context-caching-compaction.png) - ä¿¡æ¯å›¾\n",
    "\n",
    "### Day 8 å›é¡¾\n",
    "- Static vs Turn Instructions æ¨¡å¼\n",
    "- Sessionã€Memoryã€Artifacts æ¶æ„\n",
    "- Context as a Compiled View è®¾è®¡ç†å¿µ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
