"""
Day 5: Production Observability Demo

æ¼”ç¤ºå¦‚ä½•åœ¨ Agent ä¸­é›†æˆ OpenTelemetry è¿›è¡Œå¯è§‚æµ‹æ€§ç›‘æ§ã€‚
è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼Œå±•ç¤ºæ ¸å¿ƒæ¦‚å¿µã€‚

å®é™…ç”Ÿäº§ä¸­è¯·ä½¿ç”¨ Agent Starter Pack çš„å®Œæ•´ telemetry æ¨¡å—ã€‚
"""

import os
import time
import json
import logging
from datetime import datetime
from typing import Optional
from dataclasses import dataclass, field, asdict

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# ============================================================
# Span æ•°æ®ç»“æ„
# ============================================================

@dataclass
class Span:
    """è¡¨ç¤ºä¸€ä¸ªè¿½è¸ª Span"""
    name: str
    trace_id: str
    span_id: str
    parent_id: Optional[str] = None
    start_time: float = field(default_factory=time.time)
    end_time: Optional[float] = None
    attributes: dict = field(default_factory=dict)
    status: str = "OK"

    def end(self):
        self.end_time = time.time()

    @property
    def duration_ms(self) -> float:
        if self.end_time:
            return (self.end_time - self.start_time) * 1000
        return 0

    def set_attribute(self, key: str, value):
        self.attributes[key] = value

    def to_dict(self) -> dict:
        return {
            "name": self.name,
            "trace_id": self.trace_id,
            "span_id": self.span_id,
            "parent_id": self.parent_id,
            "start_time": datetime.fromtimestamp(self.start_time).isoformat(),
            "end_time": datetime.fromtimestamp(self.end_time).isoformat() if self.end_time else None,
            "duration_ms": self.duration_ms,
            "attributes": self.attributes,
            "status": self.status
        }


# ============================================================
# ç®€åŒ–ç‰ˆ Tracer
# ============================================================

class SimpleTracer:
    """ç®€åŒ–ç‰ˆè¿½è¸ªå™¨ï¼Œç”¨äºæ¼”ç¤ºæ¦‚å¿µ"""

    def __init__(self, service_name: str):
        self.service_name = service_name
        self.spans: list[Span] = []
        self._current_span: Optional[Span] = None
        self._trace_id = self._generate_id()

    def _generate_id(self) -> str:
        import random
        return hex(random.getrandbits(64))[2:]

    def start_span(self, name: str) -> Span:
        """å¼€å§‹ä¸€ä¸ªæ–°çš„ Span"""
        span = Span(
            name=name,
            trace_id=self._trace_id,
            span_id=self._generate_id(),
            parent_id=self._current_span.span_id if self._current_span else None
        )
        span.set_attribute("service.name", self.service_name)
        self.spans.append(span)
        self._current_span = span
        return span

    def end_span(self, span: Span):
        """ç»“æŸä¸€ä¸ª Span"""
        span.end()
        # æ¢å¤åˆ°çˆ¶ Span
        if span.parent_id:
            for s in reversed(self.spans):
                if s.span_id == span.parent_id:
                    self._current_span = s
                    break
        else:
            self._current_span = None

    def get_trace_summary(self) -> dict:
        """è·å–è¿½è¸ªæ‘˜è¦"""
        return {
            "trace_id": self._trace_id,
            "service": self.service_name,
            "total_spans": len(self.spans),
            "spans": [s.to_dict() for s in self.spans]
        }


# ============================================================
# GenAI Telemetry è®°å½•å™¨
# ============================================================

@dataclass
class GenAITelemetryRecord:
    """GenAI è°ƒç”¨é¥æµ‹è®°å½•"""
    timestamp: str
    model: str
    input_tokens: int
    output_tokens: int
    latency_ms: float
    status: str = "success"
    # NO_CONTENT æ¨¡å¼ä¸‹ä¸è®°å½•ä»¥ä¸‹å­—æ®µ
    # prompt: str = ""
    # response: str = ""


class GenAITelemetryLogger:
    """GenAI é¥æµ‹è®°å½•å™¨ï¼ˆæ¨¡æ‹Ÿ GCS ä¸Šä¼ ï¼‰"""

    def __init__(self, bucket_name: Optional[str] = None, capture_content: str = "NO_CONTENT"):
        self.bucket_name = bucket_name
        self.capture_content = capture_content
        self.records: list[GenAITelemetryRecord] = []

        if bucket_name:
            logger.info(f"GenAI Telemetry enabled - bucket: {bucket_name}, mode: {capture_content}")
        else:
            logger.info("GenAI Telemetry disabled (no bucket configured)")

    def log_completion(self, model: str, input_tokens: int, output_tokens: int,
                       latency_ms: float, status: str = "success"):
        """è®°å½•ä¸€æ¬¡ LLM è°ƒç”¨"""
        record = GenAITelemetryRecord(
            timestamp=datetime.utcnow().isoformat() + "Z",
            model=model,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            latency_ms=latency_ms,
            status=status
        )
        self.records.append(record)

        # æ¨¡æ‹Ÿä¸Šä¼ åˆ° GCSï¼ˆå®é™…ä¸­ä½¿ç”¨ JSONL æ ¼å¼ï¼‰
        if self.bucket_name:
            logger.info(f"[Telemetry] {model}: {input_tokens}+{output_tokens} tokens, {latency_ms:.0f}ms")

    def export_jsonl(self) -> str:
        """å¯¼å‡ºä¸º JSONL æ ¼å¼"""
        lines = []
        for record in self.records:
            lines.append(json.dumps(asdict(record)))
        return "\n".join(lines)

    def get_summary(self) -> dict:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        if not self.records:
            return {"total_calls": 0}

        total_input = sum(r.input_tokens for r in self.records)
        total_output = sum(r.output_tokens for r in self.records)
        avg_latency = sum(r.latency_ms for r in self.records) / len(self.records)

        return {
            "total_calls": len(self.records),
            "total_input_tokens": total_input,
            "total_output_tokens": total_output,
            "total_tokens": total_input + total_output,
            "avg_latency_ms": round(avg_latency, 2)
        }


# ============================================================
# æ¨¡æ‹Ÿ Agent æ‰§è¡Œ
# ============================================================

class MockLLM:
    """æ¨¡æ‹Ÿ LLM è°ƒç”¨"""

    def __init__(self, model: str = "gemini-2.0-flash"):
        self.model = model

    def generate(self, prompt: str) -> dict:
        """æ¨¡æ‹Ÿç”Ÿæˆå“åº”"""
        # æ¨¡æ‹Ÿå»¶è¿Ÿ
        time.sleep(0.1 + len(prompt) * 0.001)

        return {
            "text": f"è¿™æ˜¯å¯¹ '{prompt[:20]}...' çš„å›å¤",
            "usage": {
                "input_tokens": len(prompt) * 2,
                "output_tokens": 50 + len(prompt)
            }
        }


def simulate_agent_request(tracer: SimpleTracer, telemetry: GenAITelemetryLogger,
                           llm: MockLLM, user_input: str):
    """æ¨¡æ‹Ÿä¸€æ¬¡ Agent è¯·æ±‚å¤„ç†"""

    # ä¸» Span
    main_span = tracer.start_span("agent.handle_request")
    main_span.set_attribute("user_input_length", len(user_input))

    try:
        # è§£æè¾“å…¥
        parse_span = tracer.start_span("agent.parse_input")
        time.sleep(0.02)  # æ¨¡æ‹Ÿè§£æ
        tracer.end_span(parse_span)

        # ç¬¬ä¸€æ¬¡ LLM è°ƒç”¨ï¼ˆç†è§£æ„å›¾ï¼‰
        llm_span_1 = tracer.start_span("llm.generate")
        llm_span_1.set_attribute("model", llm.model)
        llm_span_1.set_attribute("purpose", "intent_understanding")

        start = time.time()
        response1 = llm.generate(f"ç†è§£ç”¨æˆ·æ„å›¾: {user_input}")
        latency1 = (time.time() - start) * 1000

        llm_span_1.set_attribute("input_tokens", response1["usage"]["input_tokens"])
        llm_span_1.set_attribute("output_tokens", response1["usage"]["output_tokens"])
        tracer.end_span(llm_span_1)

        telemetry.log_completion(
            model=llm.model,
            input_tokens=response1["usage"]["input_tokens"],
            output_tokens=response1["usage"]["output_tokens"],
            latency_ms=latency1
        )

        # å·¥å…·æ‰§è¡Œ
        tool_span = tracer.start_span("tool.execute")
        tool_span.set_attribute("tool_name", "search_database")
        time.sleep(0.05)  # æ¨¡æ‹Ÿå·¥å…·æ‰§è¡Œ
        tracer.end_span(tool_span)

        # ç¬¬äºŒæ¬¡ LLM è°ƒç”¨ï¼ˆç”Ÿæˆå›å¤ï¼‰
        llm_span_2 = tracer.start_span("llm.generate")
        llm_span_2.set_attribute("model", llm.model)
        llm_span_2.set_attribute("purpose", "response_generation")

        start = time.time()
        response2 = llm.generate(f"åŸºäºæœç´¢ç»“æœå›å¤: {user_input}")
        latency2 = (time.time() - start) * 1000

        llm_span_2.set_attribute("input_tokens", response2["usage"]["input_tokens"])
        llm_span_2.set_attribute("output_tokens", response2["usage"]["output_tokens"])
        tracer.end_span(llm_span_2)

        telemetry.log_completion(
            model=llm.model,
            input_tokens=response2["usage"]["input_tokens"],
            output_tokens=response2["usage"]["output_tokens"],
            latency_ms=latency2
        )

        # æ ¼å¼åŒ–å“åº”
        format_span = tracer.start_span("agent.format_response")
        time.sleep(0.01)
        tracer.end_span(format_span)

        main_span.set_attribute("status", "success")

    except Exception as e:
        main_span.set_attribute("status", "error")
        main_span.set_attribute("error.message", str(e))
        main_span.status = "ERROR"

    finally:
        tracer.end_span(main_span)


# ============================================================
# æ¼”ç¤º
# ============================================================

def main():
    print("=" * 60)
    print("Day 5: Production Observability Demo")
    print("=" * 60)
    print()

    # åˆå§‹åŒ–ç»„ä»¶
    tracer = SimpleTracer(service_name="my-agent")
    telemetry = GenAITelemetryLogger(
        bucket_name="gs://demo-bucket/logs",  # æ¨¡æ‹Ÿ bucket
        capture_content="NO_CONTENT"
    )
    llm = MockLLM(model="gemini-2.0-flash")

    # æ¨¡æ‹Ÿå¤šä¸ªè¯·æ±‚
    requests = [
        "åŒ—äº¬ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "å¸®æˆ‘æ‰¾ä¸€å®¶é™„è¿‘è¯„åˆ†é«˜çš„è¥¿é¤å…",
        "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯ OpenTelemetry"
    ]

    print("\nğŸ“¨ å¤„ç†ç”¨æˆ·è¯·æ±‚...")
    print("-" * 60)

    for req in requests:
        print(f"\nç”¨æˆ·: {req}")
        simulate_agent_request(tracer, telemetry, llm, req)

    # è¾“å‡ºè¿½è¸ªç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š Trace Summary (ç±»ä¼¼ Cloud Trace)")
    print("=" * 60)

    trace_data = tracer.get_trace_summary()
    print(f"\nTrace ID: {trace_data['trace_id']}")
    print(f"Service: {trace_data['service']}")
    print(f"Total Spans: {trace_data['total_spans']}")

    print("\nSpans:")
    for span in trace_data['spans']:
        indent = "  " if span['parent_id'] else ""
        print(f"{indent}â”œâ”€â”€ {span['name']}: {span['duration_ms']:.1f}ms")
        for key, value in span['attributes'].items():
            if key not in ['service.name']:
                print(f"{indent}â”‚   â””â”€â”€ {key}: {value}")

    # è¾“å‡ºé¥æµ‹æ‘˜è¦
    print("\n" + "=" * 60)
    print("ğŸ“ˆ GenAI Telemetry Summary (ç±»ä¼¼ BigQuery æŸ¥è¯¢ç»“æœ)")
    print("=" * 60)

    summary = telemetry.get_summary()
    print(f"""
Total LLM Calls: {summary['total_calls']}
Total Tokens: {summary['total_tokens']}
  - Input: {summary['total_input_tokens']}
  - Output: {summary['total_output_tokens']}
Avg Latency: {summary['avg_latency_ms']}ms
""")

    # è¾“å‡º JSONL æ ¼å¼
    print("=" * 60)
    print("ğŸ“„ JSONL Export (ç±»ä¼¼ GCS æ—¥å¿—æ–‡ä»¶)")
    print("=" * 60)
    print()
    print(telemetry.export_jsonl())
    print()

    print("=" * 60)
    print("âœ… Demo å®Œæˆ")
    print("=" * 60)
    print("""
å®é™…ç”Ÿäº§ä¸­ï¼š
- Traces å¯¼å‡ºåˆ° Cloud Traceï¼Œå¯è§†åŒ–æŸ¥çœ‹è°ƒç”¨é“¾
- JSONL ä¸Šä¼ åˆ° GCSï¼Œé€šè¿‡ BigQuery å¤–éƒ¨è¡¨æŸ¥è¯¢
- å¯ä»¥æŒ‰ modelã€æ—¶é—´ã€user ç­‰ç»´åº¦åˆ†æ Token æ¶ˆè€—å’Œå»¶è¿Ÿ
""")


if __name__ == "__main__":
    main()
